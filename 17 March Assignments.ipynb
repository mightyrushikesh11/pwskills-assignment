{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e1fafff-bf1d-4c38-a4fd-05859d10f340",
   "metadata": {},
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aeca25-2712-4f12-91d9-af7223caea95",
   "metadata": {},
   "source": [
    "Missing values in a dataset are values that are not present for some observations or variables. This can happen for various reasons, such as data entry errors, incomplete data, or intentional missingness.\n",
    "\n",
    "It is essential to handle missing values because they can lead to biased or inaccurate analysis and modeling results. If missing values are not handled appropriately, it can lead to invalid conclusions, wrong predictions, and biased models. Therefore, it is necessary to impute or remove missing values before performing any analysis or modeling.\n",
    "\n",
    "Some algorithms that are not affected by missing values include tree-based algorithms such as decision trees, random forests, and gradient boosting. These algorithms can handle missing values by treating them as a separate category or by splitting the data based on non-missing values. Other algorithms that can handle missing values include k-nearest neighbor (KNN), support vector machines (SVM), and Bayesian methods. However, the performance of these algorithms may depend on the amount and type of missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c36e5d7-8035-423f-94a1-36c3e08f10b8",
   "metadata": {},
   "source": [
    "Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc2709d-debb-4109-b289-00626f1d5f9b",
   "metadata": {},
   "source": [
    "There are various techniques that can be used to handle missing data in a dataset. Here are some commonly used techniques along with their examples in Python:\n",
    "\n",
    "Deletion: In this technique, we remove the rows or columns that have missing data. This is only suitable when we have a small amount of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3518c74f-0f35-4bcc-9de1-f4d40a4d198a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B   C\n",
      "0  1.0  5.0   9\n",
      "3  4.0  8.0  12\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, np.nan, 8], 'C': [9, 10, 11, 12]})\n",
    "\n",
    "# drop the rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7633797-3fab-422b-8950-18e7db2511b1",
   "metadata": {},
   "source": [
    "Imputation: In this technique, we replace the missing values with some values. One common way is to replace the missing value with the mean or median of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fae357f5-42f3-4e8e-a66a-352a54ef0866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A    B   C\n",
      "0  1.000000  5.0   9\n",
      "1  2.000000  6.5  10\n",
      "2  2.333333  6.5  11\n",
      "3  4.000000  8.0  12\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, np.nan, 8], 'C': [9, 10, 11, 12]})\n",
    "\n",
    "# replace the missing values with the mean of the column\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7423ba4a-cbfd-404c-b028-cfec11208d13",
   "metadata": {},
   "source": [
    "Interpolation: In this technique, we estimate the missing values based on the existing data by using a mathematical function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcf67743-b031-48a2-9b73-2ab6cf8356d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B   C\n",
      "0  1.0  5.0   9\n",
      "1  2.0  6.0  10\n",
      "2  3.0  7.0  11\n",
      "3  4.0  8.0  12\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, np.nan, 8], 'C': [9, 10, 11, 12]})\n",
    "\n",
    "# interpolate the missing values\n",
    "df.interpolate(inplace=True)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f23b6b2-0b8b-4103-b3c9-096750838f1f",
   "metadata": {},
   "source": [
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb94fed-2886-40b8-9105-0e2e70aeaf51",
   "metadata": {},
   "source": [
    "Imbalanced data refers to a situation where the distribution of target classes in a dataset is not equal, meaning one class is significantly more frequent than the other(s). For example, in a binary classification problem, if the positive class (or the minority class) makes up only a small portion of the dataset, it is an imbalanced dataset.\n",
    "\n",
    "If imbalanced data is not handled, it can lead to biased models that perform poorly on the minority class. This is because machine learning algorithms are designed to minimize the overall error rate, which means that they tend to focus more on the majority class since it has more samples. As a result, the minority class is often misclassified, and the model's performance on that class is poor.\n",
    "\n",
    "Moreover, if the minority class represents a critical outcome or event, such as a disease or fraud, ignoring the imbalance can lead to severe consequences in real-world scenarios. The model will not detect the minority class, and it will be difficult to make accurate predictions.\n",
    "\n",
    "Therefore, handling imbalanced data is crucial to ensure that the model can accurately classify both classes. There are various techniques to handle imbalanced data, such as oversampling the minority class, undersampling the majority class, using cost-sensitive learning algorithms, and using ensemble methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fd95c4-d9bb-4a64-baa2-bdcc8f2ac842",
   "metadata": {},
   "source": [
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69763763-2cff-40ae-b92a-6833b5349067",
   "metadata": {},
   "source": [
    "Up-sampling and down-sampling are two techniques used to address the issue of imbalanced data.\n",
    "\n",
    "Up-sampling involves increasing the number of samples in the minority class by randomly replicating existing samples, generating synthetic samples, or both. This technique helps balance the class distribution and ensures that the model does not ignore the minority class.\n",
    "\n",
    "Down-sampling involves reducing the number of samples in the majority class by randomly removing samples, selecting a subset of samples, or both. This technique helps balance the class distribution and prevent the model from being biased towards the majority class.\n",
    "\n",
    "An example of when up-sampling and down-sampling are required is in a credit card fraud detection problem. Suppose we have a dataset with 10,000 transactions, out of which only 100 are fraudulent. In this case, we have an imbalanced dataset where the minority class (fraudulent transactions) represents only 1% of the total data. If we train a model on this dataset without balancing the class distribution, the model will likely perform poorly on the minority class and will not be able to detect most fraudulent transactions.\n",
    "\n",
    "In such a scenario, we can use up-sampling and/or down-sampling to balance the class distribution. Up-sampling can be used to generate additional fraudulent transactions by using techniques such as SMOTE (Synthetic Minority Over-sampling Technique), while down-sampling can be used to reduce the number of non-fraudulent transactions. By balancing the class distribution, we can improve the model's performance on the minority class and increase the overall accuracy of the model.\n",
    "\n",
    "In summary, up-sampling and down-sampling are techniques used to handle imbalanced data, and they can be used when the class distribution is significantly skewed towards one class, as in the case of credit card fraud detection, medical diagnosis, or rare event detection problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c003c31-08d8-4b63-a735-d81bc9c2d1e8",
   "metadata": {},
   "source": [
    "Q5: What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7a0009-8382-4631-9267-195c55c158fa",
   "metadata": {},
   "source": [
    "Data augmentation is a technique used to increase the size of a dataset by creating new synthetic samples that are similar to the original data. This technique is particularly useful when the dataset is small or imbalanced, as it can help improve the performance of machine learning models by providing more data to train on.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a type of data augmentation technique that is specifically designed for imbalanced datasets. SMOTE works by creating new synthetic samples for the minority class by interpolating between existing samples. The technique selects one sample from the minority class and finds its k-nearest neighbors. It then generates new samples by creating linear combinations of the features between the selected sample and its k-nearest neighbors. This process is repeated for each sample in the minority class, resulting in a larger and more balanced dataset.\n",
    "\n",
    "For example, suppose we have a dataset with 1,000 samples, out of which only 100 belong to the minority class. We can use SMOTE to generate additional samples for the minority class by interpolating between the existing samples. If we set k=5, the algorithm will select one sample from the minority class and find its 5 nearest neighbors. It will then create new synthetic samples by taking a weighted average of the features of the selected sample and its neighbors. This process is repeated for all samples in the minority class until the desired balance is achieved.\n",
    "\n",
    "SMOTE is a powerful technique for handling imbalanced datasets as it can create new synthetic samples that are representative of the minority class. However, it is important to note that SMOTE should be used with caution as it can lead to overfitting if not applied correctly. It is also recommended to combine SMOTE with other techniques such as undersampling or cost-sensitive learning to improve the overall performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1509c2fc-ef06-4708-b32b-e4be7714dbcb",
   "metadata": {},
   "source": [
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b43c32-dfbf-4167-82b2-a1dd3e6a2967",
   "metadata": {},
   "source": [
    "Outliers in a dataset are data points that are significantly different from other data points in the same dataset. Outliers can occur due to various reasons such as measurement errors, data entry errors, or genuine extreme values. Outliers can be identified through visual inspection of the data or using statistical methods such as the z-score or the interquartile range (IQR).\n",
    "\n",
    "It is essential to handle outliers because they can have a significant impact on the performance of machine learning models. Outliers can affect the mean and variance of the data, leading to biased estimates of model parameters. Outliers can also affect the correlation between variables, leading to inaccurate predictions.\n",
    "\n",
    "Moreover, some machine learning algorithms such as linear regression are sensitive to outliers and can be heavily influenced by them. Outliers can lead to overfitting of the model, reducing its generalizability to new data.\n",
    "\n",
    "Handling outliers is necessary to ensure that machine learning models are accurate and reliable. There are various techniques for handling outliers such as:\n",
    "\n",
    "Removal of outliers: Outliers can be removed from the dataset if they are determined to be erroneous or are expected to be rare events. However, care must be taken not to remove too many data points, as this can lead to a loss of information and biased models.\n",
    "\n",
    "Imputation of outliers: Outliers can be imputed with values that are more representative of the data distribution. For example, the mean, median, or mode can be used to replace missing values.\n",
    "\n",
    "Transformation of data: Outliers can be transformed using mathematical functions such as logarithmic, exponential, or power functions. This can help bring extreme values closer to the mean and reduce the impact of outliers on the model.\n",
    "\n",
    "In summary, handling outliers is crucial to ensure that machine learning models are accurate and reliable. Outliers can affect the mean, variance, and correlation of the data, leading to biased models and inaccurate predictions. Therefore, it is essential to use appropriate techniques to handle outliers and ensure that models are trained on clean and representative data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9bc8db-a912-42c8-a03f-f4062e5eec57",
   "metadata": {},
   "source": [
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e4af21-0e76-438a-8e38-18db9fd93e9c",
   "metadata": {},
   "source": [
    "Handling missing data is essential for accurate analysis of customer data. There are several techniques that can be used to handle missing data in a dataset:\n",
    "\n",
    "Deletion: One approach to handling missing data is to simply delete the rows or columns containing missing values. This technique is only recommended if the missing data is minimal and does not significantly impact the analysis.\n",
    "\n",
    "Imputation: Imputation is the process of filling in missing data with estimates based on other available data. Common imputation techniques include mean, median, or mode imputation, which fill in missing values with the average, middle, or most frequent value of the available data. Another popular technique is K-nearest neighbor imputation, which fills in missing values based on the values of the nearest neighbors in the dataset.\n",
    "\n",
    "Regression imputation: Regression imputation is a technique that involves using regression analysis to estimate missing values based on the relationship between the missing variable and other available variables in the dataset.\n",
    "\n",
    "Multiple imputation: Multiple imputation involves generating several plausible values for each missing data point based on the available data and imputing each plausible value separately to create several completed datasets. These datasets are then analyzed separately, and the results are combined to obtain a final estimate of the missing data.\n",
    "\n",
    "Advanced machine learning techniques: Advanced machine learning techniques such as deep learning can be used to predict missing data based on patterns in the available data.\n",
    "\n",
    "It is important to carefully evaluate and choose the appropriate technique for handling missing data based on the type and amount of missing data, as well as the specific requirements of the analysis. By properly handling missing data, accurate insights can be derived from customer data, leading to better decision-making and improved outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8005a0-9807-40c9-b73a-ccf860f0aeaa",
   "metadata": {},
   "source": [
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341e2b05-03da-4dd5-8176-f4f25091e999",
   "metadata": {},
   "source": [
    "Determining whether the missing data is missing at random or if there is a pattern to the missing data is important for selecting appropriate methods for handling the missing data. Here are some strategies that can be used to determine if the missing data is missing at random or if there is a pattern to the missing data:\n",
    "\n",
    "Visual inspection: One way to determine if there is a pattern to the missing data is to visually inspect the dataset. This can be done by creating plots of the data to look for trends or patterns in the missing data.\n",
    "\n",
    "Statistical tests: There are several statistical tests that can be used to determine if there is a pattern to the missing data. For example, a chi-square test can be used to determine if the missing data is related to other variables in the dataset.\n",
    "\n",
    "Imputation: Imputation is a technique used to fill in missing data with estimates based on other available data. By imputing the missing data and comparing the results to the original dataset, it is possible to determine if there is a pattern to the missing data.\n",
    "\n",
    "Machine learning: Machine learning algorithms such as decision trees can be used to predict missing data based on other available data. By comparing the results of the machine learning algorithm to the original dataset, it is possible to determine if there is a pattern to the missing data.\n",
    "\n",
    "In summary, determining if the missing data is missing at random or if there is a pattern to the missing data is important for selecting appropriate methods for handling the missing data. Strategies such as visual inspection, statistical tests, imputation, and machine learning can be used to determine if there is a pattern to the missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb11afd7-b533-4dad-b3fe-059e50f1efdb",
   "metadata": {},
   "source": [
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b9eef7-4031-450a-a985-1bd5c98a32b8",
   "metadata": {},
   "source": [
    "In a medical diagnosis project where the majority of patients do not have the condition of interest, while a small percentage do, the dataset is imbalanced. Evaluating the performance of a machine learning model on such an imbalanced dataset requires careful consideration. Here are some strategies that can be used to evaluate the performance of the machine learning model:\n",
    "\n",
    "Confusion matrix: A confusion matrix can be used to visualize the true positive, true negative, false positive, and false negative rates of the model's predictions. This provides a quick overview of how well the model is performing and helps to identify where it is making mistakes.\n",
    "\n",
    "Precision, Recall, and F1 Score: The precision, recall, and F1 score are evaluation metrics that are commonly used to evaluate the performance of machine learning models on imbalanced datasets. These metrics take into account the number of true positive, false positive, true negative, and false negative predictions and provide a more accurate picture of the model's performance than accuracy alone.\n",
    "\n",
    "ROC Curve and AUC: The ROC (Receiver Operating Characteristic) curve is a graphical representation of the trade-off between the true positive rate and the false positive rate of a model. AUC (Area Under the Curve) is a metric that measures the overall performance of the model based on the ROC curve. A model with an AUC of 1 indicates a perfect performance, while a model with an AUC of 0.5 indicates a random guess.\n",
    "\n",
    "Class weights: Class weights can be used to adjust the importance of each class in the model. By assigning higher weights to the minority class, the model is encouraged to pay more attention to the minority class and improve its performance.\n",
    "\n",
    "Resampling techniques: Resampling techniques such as oversampling and undersampling can be used to balance the dataset by increasing the number of samples in the minority class or decreasing the number of samples in the majority class.\n",
    "\n",
    "In summary, evaluating the performance of a machine learning model on an imbalanced dataset requires careful consideration. Strategies such as confusion matrix, precision, recall, and F1 score, ROC curve and AUC, class weights, and resampling techniques can be used to evaluate the performance of the model and improve its accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d59407-6e2f-461a-9fb7-5c464275b850",
   "metadata": {},
   "source": [
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1267d1-04d7-442e-af92-f2e687d82032",
   "metadata": {},
   "source": [
    "When attempting to estimate customer satisfaction for a project, an unbalanced dataset with the bulk of customers reporting being satisfied can lead to biased results. To balance the dataset and down-sample the majority class, we can use the following methods:\n",
    "\n",
    "Undersampling: Undersampling is a technique that reduces the size of the majority class by randomly selecting a subset of the majority class samples to match the number of samples in the minority class. This can help to balance the dataset and improve the accuracy of the model.\n",
    "Here's an example of how to down-sample the majority class using Python's scikit-learn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280d3cfe-5866-47a2-8e13-0a62fe8c88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate majority and minority classes\n",
    "majority_class = df[df.satisfaction==0]\n",
    "minority_class = df[df.satisfaction==1]\n",
    " \n",
    "# Downsample majority class\n",
    "downsampled = resample(majority_class, \n",
    "                       replace=False,    # sample without replacement\n",
    "                       n_samples=len(minority_class),  # match minority n\n",
    "                       random_state=42) # reproducible results\n",
    " \n",
    "# Combine minority class with downsampled majority class\n",
    "balanced_df = pd.concat([downsampled, minority_class])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d05f63c-f7c7-4470-a841-228e68d0fc8a",
   "metadata": {},
   "source": [
    "Synthetic Minority Over-sampling Technique (SMOTE): SMOTE is a technique that creates synthetic samples of the minority class by interpolating between existing samples. This can help to balance the dataset and improve the accuracy of the model.\n",
    "Here's an example of how to use SMOTE to balance the dataset using Python's imbalanced-learn library:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e425cb8a-68d7-469b-b1c9-e1459619f068",
   "metadata": {},
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Separate majority and minority classes\n",
    "majority_class = df[df.satisfaction==0]\n",
    "minority_class = df[df.satisfaction==1]\n",
    " \n",
    "# Use SMOTE to oversample minority class\n",
    "smote = SMOTE()\n",
    "oversampled, y = smote.fit_resample(X, y)\n",
    " \n",
    "# Combine majority class with oversampled minority class\n",
    "balanced_df = pd.concat([majority_class, oversampled])\n",
    "\n",
    "In summary, to balance an unbalanced dataset with down-sampled majority class, we can use undersampling or SMOTE. Both techniques can help to balance the dataset and improve the accuracy of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f8bc23-0ae7-4b38-a904-3c50ab4d5053",
   "metadata": {},
   "source": [
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6d2786-90f9-4e54-926f-e68aac17b9ef",
   "metadata": {},
   "source": [
    "When working on a project that requires estimating the occurrence of a rare event and the dataset is unbalanced with a low percentage of occurrences, we need to up-sample the minority class to balance the dataset. Here are some methods we can employ to balance the dataset and up-sample the minority class:\n",
    "\n",
    "Oversampling: Oversampling is a technique that increases the size of the minority class by randomly duplicating existing samples. This can help to balance the dataset and improve the accuracy of the model.\n",
    "Here's an example of how to up-sample the minority class using Python's scikit-learn library:\n",
    "\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate majority and minority classes\n",
    "majority_class = df[df.target==0]\n",
    "minority_class = df[df.target==1]\n",
    " \n",
    "# Upsample minority class\n",
    "upsampled = resample(minority_class, \n",
    "                     replace=True,     # sample with replacement\n",
    "                     n_samples=len(majority_class),    # match majority n\n",
    "                     random_state=42)  # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "balanced_df = pd.concat([majority_class, upsampled])\n",
    "Synthetic Minority Over-sampling Technique (SMOTE): SMOTE is a technique that creates synthetic samples of the minority class by interpolating between existing samples. This can help to balance the dataset and improve the accuracy of the model.\n",
    "Here's an example of how to use SMOTE to balance the dataset using Python's imbalanced-learn library:\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Separate majority and minority classes\n",
    "majority_class = df[df.target==0]\n",
    "minority_class = df[df.target==1]\n",
    " \n",
    "# Use SMOTE to oversample minority class\n",
    "smote = SMOTE()\n",
    "oversampled, y = smote.fit_resample(X, y)\n",
    " \n",
    "# Combine majority class with oversampled minority class\n",
    "balanced_df = pd.concat([majority_class, oversampled])\n",
    "In summary, to balance an unbalanced dataset with up-sampled minority class, we can use oversampling or SMOTE. Both techniques can help to balance the dataset and improve the accuracy of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
